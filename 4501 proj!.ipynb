{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6797dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "import math\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import requests \n",
    "import sqlalchemy as db\n",
    "from pandas import read_parquet\n",
    "from pyarrow.parquet import ParquetDataset\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "from numba import cuda,jit\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker, scoped_session\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0f15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need, for example:\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "redius = 6378.137\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\"\n",
    "TAXI_DIR = 'taxi_files'\n",
    "QUERY_DIRECTORY = \"queries\"\n",
    "WEATHER_DIR = \"weather_files\"\n",
    "\n",
    "zones_df =gpd.read_file(r\".\\taxi_zones\\taxi_zones.shp\")\n",
    "zones_df.to_csv(\"taxi_zones.csv\")\n",
    "zones_df = zones_df.to_crs(4326)\n",
    "zones_df[\"lat\"] = zones_df.centroid.x\n",
    "zones_df[\"lon\"] = zones_df.centroid.y\n",
    "zones_data = []\n",
    "\n",
    "zones_pickupID = zones_df.drop(\n",
    "        [\"OBJECTID\",\"Shape_Leng\",\n",
    "         \"Shape_Area\",\"zone\",\n",
    "         \"borough\",\"geometry\"],\n",
    "        axis=\"columns\").rename(\n",
    "        columns={\n",
    "        \"LocationID\":\"PULocationID\",\n",
    "        \"lon\":\"pickup_latitude\",\n",
    "        \"lat\":\"pickup_longitude\"\n",
    "        })\n",
    "zones_dropoffID = zones_df.drop([\n",
    "                \"OBJECTID\",\"Shape_Leng\",\n",
    "                \"Shape_Area\",\"zone\",\n",
    "                \"borough\",\"geometry\"],\n",
    "                axis=\"columns\").rename(\n",
    "                columns={\n",
    "                \"LocationID\":\"DOLocationID\",\n",
    "                \"lon\":\"dropoff_latitude\",\n",
    "                \"lat\":\"dropoff_longitude\"\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf4d705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the distance through coordinates\n",
    "def calculate_distance(from_coord, to_coord):\n",
    "    lat1, lon1 = from_coord\n",
    "    lat2, lon2 = to_coord\n",
    "\n",
    "    diff_lat = math.radians(lat2 - lat1)\n",
    "    diff_lon = math.radians(lon2 - lon1)\n",
    "    \n",
    "    cal_dis = (math.sin(diff_lat / 2) * math.sin(diff_lat / 2) +\n",
    "             math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *\n",
    "             math.sin(diff_lon / 2) * math.sin(diff_lon / 2))\n",
    "    cal_dis = 2 * math.atan2(math.sqrt(cal_dis), math.sqrt(1 - cal_dis))\n",
    "    distance = redius * cal_dis\n",
    "\n",
    "    return distance\n",
    "    \n",
    "\n",
    "# Add the coordinate columns to the dataframe\n",
    "def add_distance_column(dataframe):\n",
    "    dataframe[\"trip_distance\"] = dataframe.apply(\n",
    "                                lambda row: calculate_distance((row.pickup_longitude, row.pickup_latitude), \n",
    "                                                               (row.dropoff_longitude, row.dropoff_latitude)), \n",
    "                                                                axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afa56c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4775df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the download links of yellow taxi from 2009-01 to 2015-06\n",
    "def find_taxi_csv_urls():\n",
    "    response = requests.get(TAXI_URL)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    url_tags = soup.find_all(\"a\", title=\"Yellow Taxi Trip Records\")\n",
    "    all_csv_urls = []\n",
    "    pattern = \"2009|2010|2011|2012|2013|2014|2015-0[1-6]\"\n",
    "\n",
    "    for url_tag in url_tags:\n",
    "        if re.findall(pattern, url_tag[\"href\"]):\n",
    "            all_csv_urls.append(url_tag[\"href\"])\n",
    "\n",
    "    return all_csv_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_filename(filename):\n",
    "    data = pd.read_parquet(filename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url:str):\n",
    "    file_name = url.split('/')[-1]\n",
    "    if not os.path.exists(f'{TAXI_DIR}/{file_name}'):\n",
    "        with open(f'{TAXI_DIR}/{file_name}', 'wb') as f:\n",
    "            res = requests.get(url)\n",
    "            f.write(res.content)\n",
    "\n",
    "    dataframe = load_data_from_filename(f'{TAXI_DIR}/{file_name}')\n",
    "    # Delete the useless columns\n",
    "    # Attention: year 2009 and 2010 have unexpected columns' names.\n",
    "    if \"2010-\" in file_name:\n",
    "        dataframe = dataframe[\n",
    "                            (dataframe[\"passenger_count\"]> 0)\n",
    "                            & (dataframe[\"total_amount\"] > 0)\n",
    "                                ]\n",
    "        \n",
    "        dataframe.drop([\n",
    "                        \"vendor_id\", \"dropoff_datetime\", \"rate_code\", \"store_and_fwd_flag\",\n",
    "                        \"payment_type\", \"fare_amount\", \"surcharge\",\n",
    "                        \"mta_tax\",\"tolls_amount\",\"passenger_count\", \n",
    "                        \"total_amount\"\n",
    "                        ], axis=1, inplace=True)\n",
    "    elif \"2009-\" in file_name:\n",
    "        dataframe = dataframe[\n",
    "                            (dataframe[\"Passenger_Count\"]> 0)\n",
    "                            & (dataframe[\"Total_Amt\"] > 0)\n",
    "                                ]\n",
    "        \n",
    "        dataframe.drop([\n",
    "                        \"vendor_name\", \"Trip_Dropoff_DateTime\", \"Rate_Code\",\n",
    "                        \"store_and_forward\", \"Payment_Type\", \"Fare_Amt\", \n",
    "                        \"surcharge\", \"mta_tax\",\"Tolls_Amt\",\n",
    "                        \"Passenger_Count\",  'Total_Amt'\n",
    "                        ], axis=1, inplace=True)\n",
    "        \n",
    "    else:\n",
    "        dataframe = dataframe[\n",
    "                            (dataframe[\"passenger_count\"]> 0)\n",
    "                            & (dataframe[\"total_amount\"] > 0)\n",
    "                                ]\n",
    "        \n",
    "        dataframe.drop([\n",
    "                        \"RatecodeID\", \"tolls_amount\", \"payment_type\", \n",
    "                        \"store_and_fwd_flag\",\"mta_tax\", \"improvement_surcharge\", \n",
    "                        \"fare_amount\", \"extra\",\"congestion_surcharge\",\n",
    "                        \"airport_fee\",\"VendorID\", \"tpep_dropoff_datetime\", \n",
    "                        \"passenger_count\", \"total_amount\"\n",
    "                        ], axis=1, inplace=True)\n",
    "                        \n",
    "        # Unify the columns' names of year 2009 and 2012\n",
    "    if \"2010-\" in file_name or \"2009-\" in file_name:\n",
    "         dataframe.columns = [\n",
    "                            \"pickup_datetime\", \"trip_distance\", \"pickup_longitude\", \n",
    "                             \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \n",
    "                             \"tip_amount\"\n",
    "                            ]\n",
    "            \n",
    "    else:\n",
    "        # Change the IDs to the pick-up and drop-off coordinates, adding columns\n",
    "        df = pd.merge(dataframe, zones_pickupID, on = \"PULocationID\")\n",
    "        dataframe = pd.merge(df, zones_dropoffID, on = \"DOLocationID\")\n",
    "        dataframe.drop([\"PULocationID\", \"DOLocationID\"], axis=1, inplace=True)\n",
    "        dataframe.columns = [\n",
    "                            \"pickup_datetime\", \"trip_distance\", \"tip_amount\",\n",
    "                            \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                            \"tip_amount\"\n",
    "                            ]\n",
    "    \n",
    "    # Remove the invalide datas\n",
    "    dataframe = dataframe[\n",
    "                            (dataframe[\"pickup_longitude\"] >= -74.242330)\n",
    "                            & (dataframe[\"pickup_latitude\"] <= 40.908524)\n",
    "                            & (dataframe[\"pickup_longitude\"] <= -73.717047)\n",
    "                            & (dataframe[\"pickup_latitude\"] >= 40.560445)\n",
    "                            & (dataframe[\"dropoff_longitude\"] >= -74.24233)\n",
    "                            & (dataframe[\"dropoff_latitude\"] <= 40.908524)\n",
    "                            & (dataframe[\"dropoff_longitude\"] <= -73.717047)\n",
    "                            & (dataframe[\"dropoff_latitude\"] >= 40.560445)\n",
    "                            ]\n",
    "    \n",
    "    # Randomly choose a sample from every month.\n",
    "    # Accumulated sample size is approximately to the uber data amount\n",
    "    dataframe = dataframe.sample(n=2600)\n",
    "    \n",
    "    return dataframe                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_csv_urls = find_taxi_csv_urls()\n",
    "    df_list = []\n",
    "    for csv_url in all_csv_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "        add_distance_column(dataframe)\n",
    "        dataframe = dataframe[dataframe[\"trip_distance\"]> 0]\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        _df = dataframe\n",
    "        _df.to_sql(\"taxi_trips\", con=engine, index=False, if_exists=\"append\")\n",
    "        df_list.append(_df)\n",
    "    \n",
    "    \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    taxi_data.to_csv(f'finally_taxi_data.csv', index=False, header=taxi_data.columns, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.drop(columns = [\"key\", \"Unnamed: 0\"])\n",
    "    df = df[\n",
    "            (df[\"pickup_longitude\"] >= -74.242330)\n",
    "            & (df[\"pickup_latitude\"] <= 40.908524)\n",
    "            & (df[\"pickup_longitude\"] <= -73.717047)\n",
    "            & (df[\"pickup_latitude\"] >= 40.560445)\n",
    "            & (df[\"dropoff_longitude\"] >= -74.24233)\n",
    "            & (df[\"dropoff_latitude\"] <= 40.908524)\n",
    "            & (df[\"dropoff_longitude\"] <= -73.717047)\n",
    "            & (df[\"dropoff_latitude\"] >= 40.560445)\n",
    "            & (df[\"passenger_count\"] > 0)\n",
    "            & (df[\"fare_amount\"] > 0 )\n",
    "            ]\n",
    "\n",
    "    df.drop([\"passenger_count\",\"fare_amount\"], axis=1, inplace=True)\n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"]).apply(lambda t: t.replace(tzinfo=None))\n",
    "    \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    uber_dataframe= uber_dataframe[uber_dataframe[\"trip_distance\"] > 0]\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_uber_data_to_db():\n",
    "    uber_data = get_uber_data()\n",
    "    uber_data.to_csv(f'finally_uber_data.csv', index=False, header=uber_data.columns, encoding='utf-8')\n",
    "    \n",
    "    for i in range(40):\n",
    "        uber_df = uber_data[i * 5000:(i + 1)*5000]\n",
    "        uber_df.to_sql('uber_trips', con=engine, index=False, if_exists='append')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc786706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df2 = df.loc[:, [\"DATE\", \"HourlyWindSpeed\",'HourlyPrecipitation']]\n",
    "    df2[\"DATE HOUR\"] = df2[\"DATE\"].apply(lambda x:x[:-6])\n",
    "    df2.drop(columns = [\"DATE\"], axis=1, inplace=True)\n",
    "    df2[\"HourlyPrecipitation\"].replace(\"T\", 0.001)\n",
    "    \n",
    "    df2[\"HourlyWindSpeed\"] = df2[\"HourlyWindSpeed\"].apply(pd.to_numeric, errors = \"coerce\")\n",
    "    df2[\"HourlyPrecipitation\"] = df2[\"HourlyPrecipitation\"].apply(pd.to_numeric, errors = \"coerce\")\n",
    "    \n",
    "    df2[\"HourlyWindSpeed\"].fillna(0, inplace=True)\n",
    "    df2[\"HourlyPrecipitation\"].fillna(0, inplace=True)\n",
    "    \n",
    "    df3= df2.groupby([\"DATE HOUR\"], as_index=False)[\"HourlyWindSpeed\"].mean()\n",
    "    \n",
    "    df4= df2.groupby([\"DATE HOUR\"], as_index=False)[\"HourlyPrecipitation\"].sum()\n",
    "    \n",
    "    df4= pd.merge(df3, df4, on=\"DATE HOUR\", how=\"inner\")\n",
    "    \n",
    "    df4[\"DATE HOUR\"] = list(map(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%dT%H\").strftime(\"%Y-%m-%d %H:\"), df4[\"DATE HOUR\"]))\n",
    "    \n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4daff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df2 = df.loc[:, [\"DATE\", \"HourlyWindSpeed\",'HourlyPrecipitation']]\n",
    "    df2[\"DATE DAYLY\"] = df2[\"DATE\"].apply(lambda x:x[:-9])\n",
    "    df2.drop(columns=[\"DATE\"],axis= 1,inplace=True)\n",
    "    df2[\"HourlyPrecipitation\"].replace('T', 0.001)\n",
    "    \n",
    "    df2[\"HourlyWindSpeed\"] = df2[\"HourlyWindSpeed\"].apply(pd.to_numeric, errors = \"coerce\")\n",
    "    df2[\"HourlyPrecipitation\"] = df2[\"HourlyPrecipitation\"].apply(pd.to_numeric, errors = \"coerce\")\n",
    "    \n",
    "    \n",
    "    df2[\"HourlyWindSpeed\"].fillna(0, inplace=True)\n",
    "    df2[\"HourlyPrecipitation\"].fillna(0, inplace=True)\n",
    "        \n",
    "    df3= df2.groupby([\"DATE DAYLY\"], as_index=False)[\"HourlyWindSpeed\"].mean()\n",
    "    \n",
    "    df4= df2.groupby([\"DATE DAYLY\"], as_index=False)[\"HourlyPrecipitation\"].sum()\n",
    "    \n",
    "    df4= pd.merge(df3, df4, on=\"DATE DAYLY\", how='inner')\n",
    "    df4[\"DatelyWindSpeed\"] = df4[\"HourlyWindSpeed\"]\n",
    "    df4[\"DatelyPrecipitation\"] = df4[\"HourlyPrecipitation\"]\n",
    "    \n",
    "    \n",
    "    df4.drop(columns=[\"HourlyPrecipitation\",\"HourlyWindSpeed\"],axis= 1,inplace=True)\n",
    "    \n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047216cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "\n",
    "    weather_csv_files = [\n",
    "                       \"2009_weather.csv\",\"2010_weather.csv\",\"2011_weather.csv\",\n",
    "                       \"2012_weather.csv\",\"2013_weather.csv\",\"2014_weather.csv\",\n",
    "                       \"2015_weather.csv\"\n",
    "                        ]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    daily_data.drop(daily_data.loc[daily_data['DATE DAYLY'] > \"2015-06-31\"].index, inplace=True)\n",
    "    hourly_data.drop(hourly_data.loc[hourly_data['DATE HOUR'] > \"2015-06-31\"].index, inplace=True)\n",
    "    \n",
    "    hourly_data.columns = [\"datetime\", \"wind_speed\", \"precipitation\"]\n",
    "    daily_data.columns = [\"datetime\", \"wind_speed\", \"precipitation\"]\n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weather_data_to_db():\n",
    "    hourly_data, daily_data = load_and_clean_weather_data()\n",
    "\n",
    "    daily_data.to_sql(\"daily_weather\", con=engine, index=False, if_exists=\"append\")\n",
    "    daily_data.to_csv(f'finally_daily_weather_data.csv', index=False, header=daily_data.columns, encoding=\"utf-8\")\n",
    "\n",
    "    # Import the hourly weather data to the table in 13 batches and\n",
    "    # every time has amount of around 5000\n",
    "    hourly_data.to_csv(f'finally_hourly_weather_data.csv', index=False, header=hourly_data.columns, encoding=\"utf-8\")\n",
    "    for i in range(13):\n",
    "        hourly_df = hourly_data[i * 5000:(i + 1)*5000]\n",
    "        hourly_df.to_sql(\"hourly_weather\", con=engine, index=False, if_exists='append')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbc2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf410f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:29:29) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
