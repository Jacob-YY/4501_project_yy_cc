{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6797dec9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mKernel Python 3.9.6 不可用。有关详细信息，请查看 Jupyter 输出选项卡。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "import math\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests \n",
    "import sqlalchemy as db\n",
    "from pandas import read_parquet\n",
    "from pyarrow.parquet import ParquetDataset\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "from numba import cuda,jit\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker, scoped_session\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0f15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need, for example:\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "# add other constants to refer to any local data, e.g. uber & weather\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "redius = 6378.137\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\"\n",
    "TAXI_DIR = 'taxi_files'\n",
    "QUERY_DIRECTORY = \"queries\"\n",
    "WEATHER_DIR = \"weather_files\"\n",
    "\n",
    "zones_df =gpd.read_file(r\".\\taxi_zones\\taxi_zones.shp\")\n",
    "zones_df.to_csv(\"taxi_zones.csv\")\n",
    "zones_df = zones_df.to_crs(4326)\n",
    "zones_df[\"lat\"] = zones_df.centroid.x\n",
    "zones_df[\"lon\"] = zones_df.centroid.y\n",
    "zones_data = []\n",
    "\n",
    "zones_pickupID = zones_df.drop(\n",
    "        [\"OBJECTID\",\"Shape_Leng\",\n",
    "         \"Shape_Area\",\"zone\",\n",
    "         \"borough\",\"geometry\"],\n",
    "        axis=\"columns\").rename(\n",
    "        columns={\n",
    "        \"LocationID\":\"PULocationID\",\n",
    "        \"lon\":\"pickup_latitude\",\n",
    "        \"lat\":\"pickup_longitude\"\n",
    "        })\n",
    "zones_dropoffID = zones_df.drop([\n",
    "                \"OBJECTID\",\"Shape_Leng\",\n",
    "                \"Shape_Area\",\"zone\",\n",
    "                \"borough\",\"geometry\"],\n",
    "                axis=\"columns\").rename(\n",
    "                columns={\n",
    "                \"LocationID\":\"DOLocationID\",\n",
    "                \"lon\":\"dropoff_latitude\",\n",
    "                \"lat\":\"dropoff_longitude\"\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf4d705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the distance through coordinates\n",
    "def calculate_distance(from_coord, to_coord):\n",
    "    lat1, lon1 = from_coord\n",
    "    lat2, lon2 = to_coord\n",
    "\n",
    "    diff_lat = math.radians(lat2 - lat1)\n",
    "    diff_lon = math.radians(lon2 - lon1)\n",
    "    \n",
    "    cal_dis = (math.sin(diff_lat / 2) * math.sin(diff_lat / 2) +\n",
    "             math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) *\n",
    "             math.sin(diff_lon / 2) * math.sin(diff_lon / 2))\n",
    "    cal_dis = 2 * math.atan2(math.sqrt(cal_dis), math.sqrt(1 - cal_dis))\n",
    "    distance = redius * cal_dis\n",
    "\n",
    "    return distance\n",
    "    \n",
    "\n",
    "# Add the coordinate columns to the dataframe\n",
    "def add_distance_column(dataframe):\n",
    "    dataframe[\"trip_distance\"] = dataframe.apply(\n",
    "                                lambda row: calculate_distance((row.pickup_longitude, row.pickup_latitude), \n",
    "                                                               (row.dropoff_longitude, row.dropoff_latitude)), \n",
    "                                                                axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afa56c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4775df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the download links of yellow taxi from 2009-01 to 2015-06\n",
    "def find_taxi_csv_urls():\n",
    "    response = requests.get(TAXI_URL)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    url_tags = soup.find_all(\"a\", title=\"Yellow Taxi Trip Records\")\n",
    "    all_csv_urls = []\n",
    "    pattern = \"2009|2010|2011|2012|2013|2014|2015-0[1-6]\"\n",
    "\n",
    "    for url_tag in url_tags:\n",
    "        if re.findall(pattern, url_tag[\"href\"]):\n",
    "            all_csv_urls.append(url_tag[\"href\"])\n",
    "\n",
    "    return all_csv_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1e1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_filename(filename):\n",
    "    data = pd.read_parquet(filename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a8088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url:str):\n",
    "    file_name = url.split('/')[-1]\n",
    "    if not os.path.exists(f'{TAXI_DIR}/{file_name}'):\n",
    "        with open(f'{TAXI_DIR}/{file_name}', 'wb') as f:\n",
    "            res = requests.get(url)\n",
    "            f.write(res.content)\n",
    "\n",
    "    dataframe = load_data_from_filename(f'{TAXI_DIR}/{file_name}')\n",
    "    # Delete the useless columns\n",
    "    # Attention: year 2009 and 2010 have unexpected columns' names.\n",
    "    if \"2010-\" in file_name:\n",
    "        dataframe = dataframe[\n",
    "                            (dataframe[\"passenger_count\"]> 0)\n",
    "                            & (dataframe[\"total_amount\"] > 0)\n",
    "                                ]\n",
    "        \n",
    "        dataframe.drop([\n",
    "                        \"vendor_id\", \"dropoff_datetime\", \"rate_code\", \"store_and_fwd_flag\",\n",
    "                        \"payment_type\", \"fare_amount\", \"surcharge\",\n",
    "                        \"mta_tax\",\"tolls_amount\",\"passenger_count\", \n",
    "                        \"total_amount\"\n",
    "                        ], axis=1, inplace=True)\n",
    "    elif \"2009-\" in file_name:\n",
    "        dataframe = dataframe[\n",
    "                            (dataframe[\"Passenger_Count\"]> 0)\n",
    "                            & (dataframe[\"Total_Amt\"] > 0)\n",
    "                                ]\n",
    "        \n",
    "        dataframe.drop([\n",
    "                        \"vendor_name\", \"Trip_Dropoff_DateTime\", \"Rate_Code\",\n",
    "                        \"store_and_forward\", \"Payment_Type\", \"Fare_Amt\", \n",
    "                        \"surcharge\", \"mta_tax\",\"Tolls_Amt\",\n",
    "                        \"Passenger_Count\",  'Total_Amt'\n",
    "                        ], axis=1, inplace=True)\n",
    "        \n",
    "    else:\n",
    "        dataframe = dataframe[\n",
    "                            (dataframe[\"passenger_count\"]> 0)\n",
    "                            & (dataframe[\"total_amount\"] > 0)\n",
    "                                ]\n",
    "        \n",
    "        dataframe.drop([\n",
    "                        \"RatecodeID\", \"tolls_amount\", \"payment_type\", \n",
    "                        \"store_and_fwd_flag\",\"mta_tax\", \"improvement_surcharge\", \n",
    "                        \"fare_amount\", \"extra\",\"congestion_surcharge\",\n",
    "                        \"airport_fee\",\"VendorID\", \"tpep_dropoff_datetime\", \n",
    "                        \"passenger_count\", \"total_amount\"\n",
    "                        ], axis=1, inplace=True)\n",
    "                        \n",
    "        # Unify the columns' names of year 2009 and 2012\n",
    "    if \"2010-\" in file_name or \"2009-\" in file_name:\n",
    "         dataframe.columns = [\n",
    "                            \"pickup_datetime\", \"trip_distance\", \"pickup_longitude\", \n",
    "                             \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \n",
    "                             \"tip_amount\"\n",
    "                            ]\n",
    "            \n",
    "    else:\n",
    "        # Change the IDs to the pick-up and drop-off coordinates, adding columns\n",
    "        df = pd.merge(dataframe, zones_pickupID, on = \"PULocationID\")\n",
    "        dataframe = pd.merge(df, zones_dropoffID, on = \"DOLocationID\")\n",
    "        dataframe.drop([\"PULocationID\", \"DOLocationID\"], axis=1, inplace=True)\n",
    "        dataframe.columns = [\n",
    "                            \"pickup_datetime\", \"trip_distance\", \"tip_amount\",\n",
    "                            \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\",\n",
    "                            \"tip_amount\"\n",
    "                            ]\n",
    "    \n",
    "    # Remove the invalide datas\n",
    "    dataframe = dataframe[\n",
    "                            (dataframe[\"pickup_longitude\"] >= -74.242330)\n",
    "                            & (dataframe[\"pickup_latitude\"] <= 40.908524)\n",
    "                            & (dataframe[\"pickup_longitude\"] <= -73.717047)\n",
    "                            & (dataframe[\"pickup_latitude\"] >= 40.560445)\n",
    "                            & (dataframe[\"dropoff_longitude\"] >= -74.24233)\n",
    "                            & (dataframe[\"dropoff_latitude\"] <= 40.908524)\n",
    "                            & (dataframe[\"dropoff_longitude\"] <= -73.717047)\n",
    "                            & (dataframe[\"dropoff_latitude\"] >= 40.560445)\n",
    "                            ]\n",
    "    \n",
    "    # Randomly choose a sample from every month.\n",
    "    # Accumulated sample size is approximately to the uber data amount\n",
    "    dataframe = dataframe.sample(n=2600)\n",
    "    \n",
    "    return dataframe                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    all_csv_urls = find_taxi_csv_urls()\n",
    "    df_list = []\n",
    "    for csv_url in all_csv_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month_taxi_data(csv_url)\n",
    "        add_distance_column(dataframe)\n",
    "        dataframe = dataframe[dataframe[\"trip_distance\"]> 0]\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        _df = dataframe\n",
    "        _df.to_sql(\"taxi_trips\", con=engine, index=False, if_exists=\"append\")\n",
    "        df_list.append(_df)\n",
    "    \n",
    "    \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    taxi_data.to_csv(f'finally_taxi_data.csv', index=False, header=taxi_data.columns, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.drop(columns = [\"key\", \"Unnamed: 0\"])\n",
    "    df = df[\n",
    "            (df[\"pickup_longitude\"] >= -74.242330)\n",
    "            & (df[\"pickup_latitude\"] <= 40.908524)\n",
    "            & (df[\"pickup_longitude\"] <= -73.717047)\n",
    "            & (df[\"pickup_latitude\"] >= 40.560445)\n",
    "            & (df[\"dropoff_longitude\"] >= -74.24233)\n",
    "            & (df[\"dropoff_latitude\"] <= 40.908524)\n",
    "            & (df[\"dropoff_longitude\"] <= -73.717047)\n",
    "            & (df[\"dropoff_latitude\"] >= 40.560445)\n",
    "            & (df[\"passenger_count\"] > 0)\n",
    "            & (df[\"fare_amount\"] > 0 )\n",
    "            ]\n",
    "\n",
    "    df.drop([\"passenger_count\",\"fare_amount\"], axis=1, inplace=True)\n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"]).apply(lambda t: t.replace(tzinfo=None))\n",
    "    \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    uber_dataframe= uber_dataframe[uber_dataframe[\"trip_distance\"] > 0]\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_uber_data_to_db():\n",
    "    uber_data = get_uber_data()\n",
    "    uber_data.to_csv(f'finally_uber_data.csv', index=False, header=uber_data.columns, encoding='utf-8')\n",
    "    \n",
    "    for i in range(40):\n",
    "        uber_df = uber_data[i * 5000:(i + 1)*5000]\n",
    "        uber_df.to_sql('uber_trips', con=engine, index=False, if_exists='append')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc786706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df2 = df.loc[:, [\"DATE\", \"HourlyWindSpeed\",'HourlyPrecipitation']]\n",
    "    df2[\"DATE HOUR\"] = df2[\"DATE\"].apply(lambda x:x[:-6])\n",
    "    df2.drop(columns = [\"DATE\"], axis=1, inplace=True)\n",
    "    df2[\"HourlyPrecipitation\"].replace(\"T\", 0.001)\n",
    "    \n",
    "    df2[\"HourlyWindSpeed\"] = df2[\"HourlyWindSpeed\"].apply(pd.to_numeric, errors = \"coerce\")\n",
    "    df2[\"HourlyPrecipitation\"] = df2[\"HourlyPrecipitation\"].apply(pd.to_numeric, errors = \"coerce\")\n",
    "    \n",
    "    df2[\"HourlyWindSpeed\"].fillna(0, inplace=True)\n",
    "    df2[\"HourlyPrecipitation\"].fillna(0, inplace=True)\n",
    "    \n",
    "    df3= df2.groupby([\"DATE HOUR\"], as_index=False)[\"HourlyWindSpeed\"].mean()\n",
    "    \n",
    "    df4= df2.groupby([\"DATE HOUR\"], as_index=False)[\"HourlyPrecipitation\"].sum()\n",
    "    \n",
    "    df4= pd.merge(df3, df4, on=\"DATE HOUR\", how=\"inner\")\n",
    "    \n",
    "    df4[\"DATE HOUR\"] = list(map(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%dT%H\").strftime(\"%Y-%m-%d %H:\"), df4[\"DATE HOUR\"]))\n",
    "    \n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4daff35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df2 = df.loc[:, [\"DATE\", \"HourlyWindSpeed\",'HourlyPrecipitation']]\n",
    "    df2[\"DATE DAYLY\"] = df2[\"DATE\"].apply(lambda x:x[:-9])\n",
    "    df2.drop(columns=[\"DATE\"],axis= 1,inplace=True)\n",
    "    df2[\"HourlyPrecipitation\"].replace('T', 0.001)\n",
    "    \n",
    "    df2[\"HourlyWindSpeed\"] = df2[\"HourlyWindSpeed\"].apply(pd.to_numeric, errors = \"coerce\")\n",
    "    df2[\"HourlyPrecipitation\"] = df2[\"HourlyPrecipitation\"].apply(pd.to_numeric, errors = \"coerce\")\n",
    "    \n",
    "    \n",
    "    df2[\"HourlyWindSpeed\"].fillna(0, inplace=True)\n",
    "    df2[\"HourlyPrecipitation\"].fillna(0, inplace=True)\n",
    "        \n",
    "    df3= df2.groupby([\"DATE DAYLY\"], as_index=False)[\"HourlyWindSpeed\"].mean()\n",
    "    \n",
    "    df4= df2.groupby([\"DATE DAYLY\"], as_index=False)[\"HourlyPrecipitation\"].sum()\n",
    "    \n",
    "    df4= pd.merge(df3, df4, on=\"DATE DAYLY\", how='inner')\n",
    "    df4[\"DatelyWindSpeed\"] = df4[\"HourlyWindSpeed\"]\n",
    "    df4[\"DatelyPrecipitation\"] = df4[\"HourlyPrecipitation\"]\n",
    "    \n",
    "    \n",
    "    df4.drop(columns=[\"HourlyPrecipitation\",\"HourlyWindSpeed\"],axis= 1,inplace=True)\n",
    "    \n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047216cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "\n",
    "    weather_csv_files = [\n",
    "                       \"2009_weather.csv\",\"2010_weather.csv\",\"2011_weather.csv\",\n",
    "                       \"2012_weather.csv\",\"2013_weather.csv\",\"2014_weather.csv\",\n",
    "                       \"2015_weather.csv\"\n",
    "                        ]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    daily_data.drop(daily_data.loc[daily_data['DATE DAYLY'] > \"2015-06-31\"].index, inplace=True)\n",
    "    hourly_data.drop(hourly_data.loc[hourly_data['DATE HOUR'] > \"2015-06-31\"].index, inplace=True)\n",
    "    \n",
    "    hourly_data.columns = [\"datetime\", \"wind_speed\", \"precipitation\"]\n",
    "    daily_data.columns = [\"datetime\", \"wind_speed\", \"precipitation\"]\n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weather_data_to_db():\n",
    "    hourly_data, daily_data = load_and_clean_weather_data()\n",
    "\n",
    "    daily_data.to_sql(\"daily_weather\", con=engine, index=False, if_exists=\"append\")\n",
    "    daily_data.to_csv(f'finally_daily_weather_data.csv', index=False, header=daily_data.columns, encoding=\"utf-8\")\n",
    "\n",
    "    # Import the hourly weather data to the table in 13 batches and\n",
    "    # every time has amount of around 5000\n",
    "    hourly_data.to_csv(f'finally_hourly_weather_data.csv', index=False, header=hourly_data.columns, encoding=\"utf-8\")\n",
    "    for i in range(13):\n",
    "        hourly_df = hourly_data[i * 5000:(i + 1)*5000]\n",
    "        hourly_df.to_sql(\"hourly_weather\", con=engine, index=False, if_exists='append')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbc2fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mKernel Python 3.9.6 不可用。有关详细信息，请查看 Jupyter 输出选项卡。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "### part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf410f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mKernel Python 3.9.6 不可用。有关详细信息，请查看 Jupyter 输出选项卡。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "engine = db.create_engine(DATABASE_URL, echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cc6f83",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mKernel Python 3.9.6 不可用。有关详细信息，请查看 Jupyter 输出选项卡。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA =  \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS hourly_weather_data\n",
    "        (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            datetime DATETIME,\n",
    "            precipitation FLOAT,\n",
    "            wind_speed FLOAT\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS daily_weather_data\n",
    "        (\n",
    "           id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "           datetime DATETIME,\n",
    "           precipitation FLOAT,\n",
    "           wind_speed FLOAT\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS taxi_data\n",
    "        (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            pickup_datetime DATETIME,\n",
    "            trip_distance FLOAT,\n",
    "            tip_amount FLOAT,\n",
    "            pickup_longitude FLOAT,\n",
    "            pickup_latitude FLOAT,\n",
    "            dropoff_longitude FLOAT,\n",
    "            dropoff_latitude FLOAT \n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS uber_data\n",
    "        (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            pickup_datetime DATETIME,\n",
    "            trip_distance FLOAT,\n",
    "            fare_amount FLOAT,\n",
    "            pickup_longitude FLOAT,\n",
    "            pickup_latitude FLOAT,\n",
    "            dropoff_longitude FLOAT,\n",
    "            dropoff_latitude FLOAT\n",
    "        )\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9995c063",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mKernel Python 3.9.6 不可用。有关详细信息，请查看 Jupyter 输出选项卡。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f7d517",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mKernel Python 3.9.6 不可用。有关详细信息，请查看 Jupyter 输出选项卡。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    taxi_trips = connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "    uber_trips = connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "    daily_weather = connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "    hourly_weather = connection.execute(HOURLY_WEATHER_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e60f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mKernel Python 3.9.6 不可用。有关详细信息，请查看 Jupyter 输出选项卡。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "### Add data to DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc9a1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mKernel Python 3.9.6 不可用。有关详细信息，请查看 Jupyter 输出选项卡。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "get_and_clean_taxi_data()\n",
    "add_weather_data_to_db()\n",
    "add_uber_data_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9ff6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mKernel Python 3.9.6 不可用。有关详细信息，请查看 Jupyter 输出选项卡。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "pd.read_sql_table('uber_trips', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31865d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法启动 Kernel。 \n",
      "\u001b[1;31mKernel Python 3.9.6 不可用。有关详细信息，请查看 Jupyter 输出选项卡。. \n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# part 3 understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a46b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUERY_1 \n",
    "#Question 1: \n",
    "def get_hour_popular_yellow_taxi():\n",
    "    with engine.connect() as connection:\n",
    "        query_sql = f\"select strftime('%H',pickup_datetime) AS cur_hour,count(*) as num from taxi_trips GROUP BY cur_hour order by num desc;\"\n",
    "        # Write the query to the query file\n",
    "        with open(f'{QUERY_DIRECTORY}/question1.sql', \"w\") as f:\n",
    "            f.write(query_sql)\n",
    "        # Execute the query\n",
    "        result = connection.execute(query_sql)\n",
    "        return [r[0] for r in result]\n",
    "    \n",
    "print(get_hour_popular_yellow_taxi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2:\n",
    "def get_weekday_popular_uber():\n",
    "    weekdays_dict = {\n",
    "        '0': 'Sun',\n",
    "        '1': 'Mon',\n",
    "        '2': 'Tue',\n",
    "        '3': 'Wed',\n",
    "        '4': 'Thur',\n",
    "        '5': 'Fri',\n",
    "        '6': 'Sat'\n",
    "    }\n",
    "    with engine.connect() as connection:\n",
    "        query_sql = f\"select strftime('%w',pickup_datetime) AS cur_weekday,count(*) as num from uber_trips GROUP BY cur_weekday order by num desc;\"\n",
    "        # Write the query to the query file\n",
    "        with open(f'{QUERY_DIRECTORY}/question2.sql', \"w\") as f:\n",
    "            f.write(query_sql)\n",
    "        # Execute the query\n",
    "        result = connection.execute(query_sql)\n",
    "        return [weekdays_dict[r[0]] for r in result]\n",
    "    \n",
    "print(get_weekday_popular_uber())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162eb51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3\n",
    "def get_sum_trip_distence():\n",
    "    with engine.connect() as connection:\n",
    "        distence_query_sql = f\"SELECT SUM(trip_distance) FROM uber_trips where pickup_datetime like '2013-07%' UNION ALL SELECT SUM(trip_distance) FROM taxi_trips where pickup_datetime like '2013-07%';\"\n",
    "        # Write the query to the query file\n",
    "        with open(f'{QUERY_DIRECTORY}/question3.sql', \"w\") as f:\n",
    "            f.write(distence_query_sql)\n",
    "         # Execute the query\n",
    "        result = list(connection.execute(distence_query_sql))\n",
    "        distence = 0\n",
    "        for row in result:\n",
    "            if row:\n",
    "                distence += row[0]\n",
    "        return distence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b6e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第四题: \n",
    "def get_2019_trips_info():\n",
    "    with engine.connect() as connection:\n",
    "        uber_sql = f\"select strftime('%Y-%m-%d',pickup_datetime) as cur_day,COUNT(*) as trip_count,SUM(trip_distance) as trip_distance FROM uber_trips where pickup_datetime like '2009%' GROUP BY cur_day;\"\n",
    "        taxi_sql = f\"select strftime('%Y-%m-%d',pickup_datetime) as cur_day,COUNT(*) as trip_count,SUM(trip_distance) as trip_distance FROM taxi_trips where pickup_datetime like '2009%' GROUP BY cur_day;\"\n",
    "\n",
    "        with open(f'{QUERY_DIRECTORY}/question4.sql', \"w\") as f:\n",
    "            f.write(uber_sql)\n",
    "            f.write(taxi_sql)\n",
    "        uber_result = list(connection.execute(uber_sql))\n",
    "        taxi_result = list(connection.execute(taxi_sql))\n",
    "\n",
    "        trip_count_dict = {}  \n",
    "        total_trip_distance = 0  \n",
    "\n",
    "        for row in uber_result:\n",
    "            cur_day = row[0]\n",
    "            trip_count = row[1]\n",
    "            trip_distance = row[2]\n",
    "            if cur_day not in trip_count_dict:\n",
    "                trip_count_dict[cur_day] = trip_count\n",
    "            else:\n",
    "                trip_count_dict[cur_day] = trip_count_dict[cur_day] + trip_count\n",
    "            total_trip_distance += trip_distance\n",
    "\n",
    "        for row in taxi_result:\n",
    "            cur_day = row[0]\n",
    "            trip_count = row[1]\n",
    "            trip_distance = row[2]\n",
    "            if cur_day not in trip_count_dict:\n",
    "                trip_count_dict[cur_day] = trip_count\n",
    "            else:\n",
    "                trip_count_dict[cur_day] = trip_count_dict[cur_day] + trip_count\n",
    "            total_trip_distance += trip_distance\n",
    "\n",
    "        trip_count_datas = sorted(trip_count_dict.items(), key=lambda i: i[1], reverse=True)[0:10]\n",
    "        trip_count_list = [data[0] for data in trip_count_datas]\n",
    "        return trip_count_list, total_trip_distance / 10\n",
    "\n",
    "print(get_2019_trips_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f02cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第五题\n",
    "def get_windest_trips():\n",
    "    with engine.connect() as connection:\n",
    "        query_sql = \"\"\"\n",
    "        select count(*) FROM taxi_trips where strftime('%Y-%m-%d',pickup_datetime) in\n",
    "        (select datetime from daily_weather ORDER BY wind_speed desc limit 10) UNION ALL \n",
    "        select count(*) FROM uber_trips where strftime('%Y-%m-%d',pickup_datetime) in\n",
    "        (select datetime from daily_weather ORDER BY wind_speed desc limit 10);\n",
    "        \"\"\"\n",
    "\n",
    "        with open(f'{QUERY_DIRECTORY}/question5.sql', \"w\") as f:\n",
    "            f.write(query_sql)\n",
    "\n",
    "        result = list(connection.execute(query_sql))\n",
    "        total_trips = 0\n",
    "        for row in result:\n",
    "            if row:\n",
    "                total_trips += row[0]\n",
    "        return total_trips\n",
    "\n",
    "get_windest_trips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第六題：\n",
    "def get_hurricane_trips_info():\n",
    "    with engine.connect() as connection:\n",
    "\n",
    "        start_date = datetime.date(2012, 10, 22)\n",
    "        date_dict = {}\n",
    "        for i in range(16):\n",
    "            for j in range(24):\n",
    "                date_dict[f'{str(start_date)} {j:02d}'] = {\n",
    "                    'trips_num': 0, 'precipitation': 0, 'wind_speed': 0\n",
    "                }\n",
    "                j += 1\n",
    "            start_date = start_date + datetime.timedelta(days=1)\n",
    "              \n",
    "        query1 = \"\"\"\n",
    "            select strftime('%Y-%m-%d %H',pickup_datetime) as cur_hour,count(*) as\n",
    "            trips_num FROM uber_trips where pickup_datetime>='2012-10-22' and pickup_datetime<'2012-11-07' GROUP BY cur_hour;\n",
    "        \"\"\"\n",
    "        result = list(connection.execute(query1))\n",
    "        for row in result:\n",
    "            cur_hour = row[0]\n",
    "            trips_num = row[1]\n",
    "            temp_dict = date_dict[cur_hour]\n",
    "            temp_dict['trips_num'] = temp_dict['trips_num'] + trips_num\n",
    "            \n",
    "        query2 = \"\"\"\n",
    "            select strftime('%Y-%m-%d %H',pickup_datetime) as cur_hour,count(*) as\n",
    "            trips_num FROM taxi_trips where pickup_datetime>='2012-10-22' and pickup_datetime<'2012-11-07' GROUP BY cur_hour;\n",
    "        \"\"\"\n",
    "        result = list(connection.execute(query2))\n",
    "        for row in result:\n",
    "            cur_hour = row[0]\n",
    "            trips_num = row[1]\n",
    "            temp_dict = date_dict[cur_hour]\n",
    "            temp_dict['trips_num'] = temp_dict['trips_num'] + trips_num\n",
    "            \n",
    "            \n",
    "        query3 = \"\"\"\n",
    "            select * from hourly_weather where datetime>='2012-10-22' AND datetime<'2012-10-23';\n",
    "        \"\"\"\n",
    "        result = list(connection.execute(query3))\n",
    "        for row in result:\n",
    "            cur_hour = str(row[1]).replace(':', '')\n",
    "            precipitation = row[2]\n",
    "            wind_speed = row[3]\n",
    "            temp_dict = date_dict[cur_hour]\n",
    "            temp_dict['precipitation'] = precipitation\n",
    "            temp_dict['wind_speed'] = wind_speed\n",
    "        \n",
    "\n",
    "        with open(f'{QUERY_DIRECTORY}/question6.sql', \"w\") as f:\n",
    "            f.write(query1)\n",
    "            f.write(query2)\n",
    "            f.write(query3)\n",
    "        \n",
    "        return date_dict\n",
    "    \n",
    "get_hurricane_trips_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
